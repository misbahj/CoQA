# -*- coding: utf-8 -*-
"""CoQA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J_gkpwYJERNtUJjtYOwD4LvnfNTUa6TA

**PROJECT:  Question answering with a fine-tuned BERT**

Large language models, like ChatGPT, have brought enthusiasm to solving a huge variety of NLP tasks, including question answering. Asking a question and obtaining an answer quickly from a large language model can really speed up the work of people and focus on other challenging tasks.In this project, you will fine-tune BERT on the CoQA dataset, which consists of a collection of
127 thousand questions with answers released by Stanford in 2019. The goal is to use the BERT model to answer questions based on the dataset provided.
"""

#import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

!pip install transformers

import torch
from transformers import BertForQuestionAnswering
from transformers import BertTokenizer

#import CoQA dataset pretrained fined tuned
coqa = pd.read_json('http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json')
coqa.head()

del coqa["version"]

cols = ["text","question","answer"]

# j = 1
comp_list = []
for index, row in coqa.iterrows():
    for i in range(len(row["data"]["questions"])):
        temp_list = []
#         temp_list.append(j)
        temp_list.append(row["data"]["story"])
        temp_list.append(row["data"]["questions"][i]["input_text"])
        temp_list.append(row["data"]["answers"][i]["input_text"])
        comp_list.append(temp_list)
#     j += 1
new_df = pd.DataFrame(comp_list, columns=cols)

new_df.to_csv("CoQA_data.csv", index=False)

data = pd.read_csv("CoQA_data.csv")
data.head()

print("Number of question and answers: ", len(data))

model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

random_num = np.random.randint(0,len(data))

question = data["question"][random_num]
text = data["text"][random_num]

print(question, "\n", text)

input_ids = tokenizer.encode(question, text)
print("The input has a total of {} tokens.".format(len(input_ids)))

tokens = tokenizer.convert_ids_to_tokens(input_ids)

for token, id in zip(tokens, input_ids):
    print('{:8}{:8,}'.format(token,id))

#first occurence of [SEP] token
sep_idx = input_ids.index(tokenizer.sep_token_id)
print(sep_idx)

#number of tokens in segment A - question
num_seg_a = sep_idx+1
print(num_seg_a)

#number of tokens in segment B - text
num_seg_b = len(input_ids) - num_seg_a
print(num_seg_b)

segment_ids = [0]*num_seg_a + [1]*num_seg_b
print(segment_ids)

assert len(segment_ids) == len(input_ids)

#token input_ids to represent the input
#token segment_ids to differentiate our segments - text and question
output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))
#print(output.start_logits, output.end_logits)

#tokens with highest start and end scores
answer_start = torch.argmax(output.start_logits)
answer_end = torch.argmax(output.end_logits)
#print(answer_start, answer_end)

if answer_end >= answer_start:
    answer = " ".join(tokens[answer_start:answer_end+1])
else:
    print("I am unable to find the answer to this question. Can you please ask another question?")

print("Text:\n{}".format(text.capitalize()))
print("\nQuestion:\n{}".format(question.capitalize()))
print("\nAnswer:\n{}.".format(answer.capitalize()))

start_scores = output.start_logits.detach().numpy().flatten()
end_scores = output.end_logits.detach().numpy().flatten()

token_labels = []
for i, token in enumerate(tokens):
    token_labels.append("{}-{}".format(token,i))

print(len(token_labels))

#first 100 tokens
plt.rcParams["figure.figsize"] = (20,10)
ax = sns.barplot(x=token_labels[:80], y=start_scores[:80], ci=None)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha="center")
ax.grid(True)
plt.title("Start word scores")
plt.show()

#last 100 tokens
plt.rcParams["figure.figsize"] = (20,10)
ax = sns.barplot(x=token_labels[-80:], y=start_scores[-80:], ci=None)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha="center")
ax.grid(True)
plt.title("Start word scores")
plt.show()

answer = tokens[answer_start]

for i in range(answer_start+1, answer_end+1):
    if tokens[i][0:2] == "##":
        answer += tokens[i][2:]
    else:
        answer += " " + tokens[i]

def question_answer(question, text):

    #tokenize question and text in ids as a pair
    input_ids = tokenizer.encode(question, text)

    #string version of tokenized ids
    tokens = tokenizer.convert_ids_to_tokens(input_ids)

    #segment IDs
    #first occurence of [SEP] token
    sep_idx = input_ids.index(tokenizer.sep_token_id)

    #number of tokens in segment A - question
    num_seg_a = sep_idx+1

    #number of tokens in segment B - text
    num_seg_b = len(input_ids) - num_seg_a

    #list of 0s and 1s
    segment_ids = [0]*num_seg_a + [1]*num_seg_b

    assert len(segment_ids) == len(input_ids)

    #model output using input_ids and segment_ids
    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))

    #reconstructing the answer
    answer_start = torch.argmax(output.start_logits)
    answer_end = torch.argmax(output.end_logits)

    if answer_end >= answer_start:
        answer = tokens[answer_start]
        for i in range(answer_start+1, answer_end+1):
            if tokens[i][0:2] == "##":
                answer += tokens[i][2:]
            else:
                answer += " " + tokens[i]

    if answer.startswith("[CLS]"):
        answer = "Unable to find the answer to your question."

#     print("Text:\n{}".format(text.capitalize()))
#     print("\nQuestion:\n{}".format(question.capitalize()))
    print("\nAnswer:\n{}".format(answer.capitalize()))
text = """"Harry Potter and the Philosopher's Stone is a fantasy novel written by British author J. K. Rowling. The book follows the story of a young wizard, Harry Potter, who discovers his magical heritage as he attends Hogwarts School of Witchcraft and Wizardry."""

question_answer(question, text)

print("Original answer:\n", data.loc[data["question"] == question]["answer"].values[0])

torch.__version__

# Function to get answer from the model
def get_answer(question, context):
    encoding = tokenizer.encode_plus(question, context, return_tensors="pt", max_length=512, truncation=True)
    input_ids = encoding["input_ids"].to(model.device)
    attention_mask = encoding["attention_mask"].to(model.device)
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits
    answer_start = torch.argmax(answer_start_scores)
    answer_end = torch.argmax(answer_end_scores) + 1
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end]))
    return answer

# Input loop
text = input("Please enter your text: \n")
question = input("\nPlease enter your question: \n")

while True:
    print("\nAnswer:", get_answer(question, text))

    flag = True
    flag_N = False

    while flag:
        response = input("\nDo you want to ask another question based on this text (Y/N)? ")
        if response[0].upper() == "Y":
            question = input("\nPlease enter your question: \n")
            flag = False
        elif response[0].upper() == "N":
            print("\nBye!")
            flag = False
            flag_N = True

    if flag_N:
        break

!pip install gradio



# Import necessary modules
import torch
from transformers import BertTokenizer, BertForQuestionAnswering
import gradio as gr

# Load the pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForQuestionAnswering.from_pretrained("bert-base-uncased")

# Define the get_answer function
def get_answer(context, question):
    # Tokenize the context and question
    input_ids = tokenizer.encode(question, context, return_tensors="pt")

import torch
from transformers import BertTokenizer, BertForQuestionAnswering
import gradio as gr

# Load the pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForQuestionAnswering.from_pretrained("bert-base-uncased")

# Function to get answer from the model
def get_answer(question, context):
    encoding = tokenizer.encode_plus(question, context, return_tensors="pt", max_length=512, truncation=True)
    input_ids = encoding["input_ids"].to(model.device)
    attention_mask = encoding["attention_mask"].to(model.device)
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits
    answer_start = torch.argmax(answer_start_scores)
    answer_end = torch.argmax(answer_end_scores) + 1
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end]))
    return answer

# Define Gradio inputs and outputs
context_input = gr.Textbox(lines=5, label="Context")
question_input = gr.Textbox(label="Question")
output_text = gr.Textbox(label="Answer")

inputs = [
    gr.Textbox(lines=5, label="Context"),
    gr.Textbox(lines=1, label="Question")
]

output = gr.Textbox(label="Answer")

interface = gr.Interface(get_answer, inputs, output, title="CoQA ChatBot", description="Enter a question and context to get the answer.")
interface.launch()

!pip install transformers

!pip install datasets

from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration
from datasets import load_dataset

# Load CoQA dataset
coqa_dataset = load_dataset("coqa")

import pandas as pd
# Convert dataset to pandas DataFrame
df = pd.DataFrame(coqa_dataset["train"])

# Display the first few examples
print(df.head())

!pip install faiss-cpu

!pip install transformers datasets

from transformers import RagTokenizer, RagRetriever

# Initialize RAG tokenizer
tokenizer = RagTokenizer.from_pretrained("facebook/rag-token-base")

import torch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline
from datasets import load_dataset

# Load CoQA dataset
coqa_dataset = load_dataset("coqa")

# Load pretrained model and tokenizer
model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

# Example question
question = "What is the answer?"

# Example context
context = "Context: " + coqa_dataset["train"][0]["story"]

# Tokenize inputs
inputs = tokenizer(question, context, return_tensors="pt")

# Perform question answering
start_positions, end_positions = model(**inputs)

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("question-answering", model="google-bert/bert-large-uncased-whole-word-masking-finetuned-squad")

# Load model directly
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-large-uncased-whole-word-masking-finetuned-squad")
model = AutoModelForQuestionAnswering.from_pretrained("google-bert/bert-large-uncased-whole-word-masking-finetuned-squad")

import requests
API_URL = "https://api-inference.huggingface.co/models/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad"
headers = {"Authorization": f"Bearer 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"}

def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()


output = query({
	"inputs": {
		"question": "What is my name?",
		"context": "My name is Clara and I live in Berkeley."
	},
})



!pip install gradio

import gradio as gr

gr.load("models/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad").launch()

!pip install openai

import os
import openai

# Set your OpenAI API key as an environment variable
os.environ["OPENAI_API_KEY"] = "sk-...RHgV"

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text2text-generation", model="eliasws/openApiT5-to-json-v3")

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("eliasws/openApiT5-to-json-v3")
model = AutoModelForSeq2SeqLM.from_pretrained("eliasws/openApiT5-to-json-v3")

import requests

API_URL = "https://api-inference.huggingface.co/models/eliasws/openApiT5-to-json-v3"
headers = {"Authorization": "Bearer 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"}

def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
	"inputs": "The answer to the universe is",
})

!pip install gradio

import gradio as gr

gr.load("models/eliasws/openApiT5-to-json-v3").launch()

# 1. Prepare the CoQA Dataset

# Code to preprocess the CoQA dataset and convert it into feature vectors

# 2. Fine-tune BERT

# Code to fine-tune BERT on the CoQA dataset and save the trained model

# 3. Deploy with FastAPI

!pip install fastapi uvicorn transformers torch

# Load fine-tuned BERT model and tokenizer
model_path = "google-bert/bert-large-uncased-whole-word-masking-finetuned-squad"
model = BertForQuestionAnswering.from_pretrained(model_path)
tokenizer = BertTokenizer.from_pretrained(model_path)

print(inputs)

print(model.config)

!pip install fastapi nest-asyncio pyngrok uvicorn

!ngrok config add-authtoken 2cXIpGuw0mL2XlFhqkhaNgZV138_zZLnCDJGYNN3eq3vxnev

from fastapi import FastAPI
from pydantic import BaseModel
from transformers import BertForQuestionAnswering, BertTokenizer
import torch

app = FastAPI()

# Load fine-tuned BERT model and tokenizer
model_path = "google-bert/bert-large-uncased-whole-word-masking-finetuned-squad"
model = BertForQuestionAnswering.from_pretrained(model_path)
tokenizer = BertTokenizer.from_pretrained(model_path)

class QAInput(BaseModel):
    question: str
    context: str

@app.post("/predict")
async def predict_qa(input: QAInput):
    # Tokenize input
    inputs = tokenizer(input.question, input.context, return_tensors="pt", padding=True, truncation=True)

    # Perform inference
    with torch.no_grad():
        outputs = model(**inputs)

    # Process output
    start_idx = torch.argmax(outputs.start_logits)
    end_idx = torch.argmax(outputs.end_logits)
    answer = tokenizer.decode(inputs.input_ids[0][start_idx:end_idx+1], skip_special_tokens=True)

    return {"answer": answer}

@app.get("/")
def read_root():
    return {"message": "Welcome to the CoQA BERT QA API"}

if __name__ == "__main__":
    import nest_asyncio
    from pyngrok import ngrok
    import uvicorn

    # Start ngrok tunnel
    ngrok_tunnel = ngrok.connect(8000)
    print('Public URL:', ngrok_tunnel.public_url)

    # Allow for handling asynchronous events in the notebook environment
    nest_asyncio.apply()

    # Start FastAPI server
    uvicorn.run(app, host='0.0.0.0', port=8000)

from fastapi import FastAPI, Query
from transformers import BertForQuestionAnswering, BertTokenizer
import torch

app = FastAPI()

# Load fine-tuned BERT model and tokenizer
model_path = "google-bert/bert-large-uncased-whole-word-masking-finetuned-squad"
model = BertForQuestionAnswering.from_pretrained(model_path)
tokenizer = BertTokenizer.from_pretrained(model_path)

class QAOutput(BaseModel):
    answer: str

@app.get("/")
def read_root():
    return {"message": "Welcome to the BERT QA API"}

@app.get("/qa")
async def get_qa(question: str = Query(..., title="Question"), context: str = Query(..., title="Context")):
    # Tokenize input
    inputs = tokenizer(question, context, return_tensors="pt", padding=True, truncation=True)

    # Perform inference
    with torch.no_grad():
        outputs = model(**inputs)

    # Process output
    start_idx = torch.argmax(outputs.start_logits)
    end_idx = torch.argmax(outputs.end_logits)
    answer = tokenizer.decode(inputs.input_ids[0][start_idx:end_idx+1], skip_special_tokens=True)

    return {"question": question, "context": context, "answer": answer}

if __name__ == "__main__":
    import nest_asyncio
    from pyngrok import ngrok
    import uvicorn

    # Start ngrok tunnel
    ngrok_tunnel = ngrok.connect(8000)
    print('Public URL:', ngrok_tunnel.public_url)

    # Allow for handling asynchronous events in the notebook environment
    nest_asyncio.apply()

    # Start FastAPI server
    uvicorn.run(app, host='0.0.0.0', port=8000)

import requests

# Define the URL of your FastAPI server's /qa endpoint
url = "http://127.0.0.1:8000 /qa"
# Define the question and context
question = "What is the meaning of life?"
context = "Life is a precious gift that should be cherished and lived to the fullest. It is a journey of self-discovery, learning, and growth. The meaning of life is unique to each individual and can be found through exploration, reflection, and the pursuit of one's passions."

# Send a GET request to the FastAPI server
try:
    response = requests.get(url, params={"question": question, "context": context})
    response.raise_for_status()
except requests.exceptions.RequestException as e:
    print(f"An error occurred: {e}")
else:
    # Parse the JSON response
    data = response.json()
    print("Answer:", data["answer"])

!http://127.0.0.1:8000/qa

!pip install openai

import openai

# Set your OpenAI API key
openai.api_key = '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'

# Define a function to generate answers using the OpenAI API
def generate_answer(question, context):
    prompt = f"Question: {question}\nContext: {context}\nAnswer:"
    response = openai.Completion.create(
        engine="davinci",
        prompt=prompt,
        max_tokens=350  # Adjust as needed
    )
    return response.choices[0].text.strip()

from transformers import BertLMHeadModel

# Load the pre-trained BertLMHeadModel
model = BertLMHeadModel.from_pretrained("bert-base-uncased")

question = "What is the main character's name in the book Harry Potter and the Sorcerer's Stone?"
context = "Harry Potter and the Philosopher's Stone is a fantasy novel written by British author J. K. Rowling. The book follows the story of a young wizard, Harry Potter, who discovers his magical heritage as he attends Hogwarts School of Witchcraft and Wizardry."

# Concatenate question and context for generation
input_text = f"Question: {question}\nContext: {context}\nAnswer:"

# Tokenize input
inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)

# Perform generation
with torch.no_grad():
    outputs = model.generate(**inputs)

# Decode the generated answer
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Print the answer
print(f"Answer: {answer}")